# -*- coding: utf-8 -*-
"""aml1.0

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BCa_igMzyjgtSPckke3WZ_yYaI0j6Vg6
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

import random
import csv
import pandas as pd
import numpy as np
import torch
from tqdm.notebook import tqdm

from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import AdamW, get_linear_schedule_with_warmup

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from torch.utils.data import TensorDataset

pwd

pd. set_option('display.max_rows', None, 'display.max_columns', None)

train = pd.read_csv('drive/MyDrive/data/dataset52a7b21/dataset/train.csv', escapechar = "\\", quoting = csv.QUOTE_NONE)

train.head()

train.info(verbose=True, null_counts=True)

len(train['BROWSE_NODE_ID'].unique())

check = pd.get_dummies(train.iloc[:1000000,:].BROWSE_NODE_ID)

len(check.columns[:-1])

needed_Class = []
for i in train['BROWSE_NODE_ID'].value_counts().to_frame().index:
  if train['BROWSE_NODE_ID'].value_counts().to_frame().loc[i]['BROWSE_NODE_ID'] >= 1000:
    needed_Class.append(i)

train = train.loc[train['BROWSE_NODE_ID'].isin(needed_Class)]

train['COMBINE'] = train['TITLE'].fillna('') + train['DESCRIPTION'].fillna('') + train['BULLET_POINTS'].fillna('') + train['BRAND'].fillna('')

train = train.loc[train['COMBINE']!='']

train.info(verbose=True, null_counts=True)

X_train, _, y_train, _ = train_test_split(train, 
                                                  train.BROWSE_NODE_ID.values, 
                                                  test_size=0.7, 
                                                  random_state=42, 
                                                  stratify=train.BROWSE_NODE_ID.values)

x_train, x_val, y_train, y_val = train_test_split(X_train.COMBINE.values, 
                                                  X_train.BROWSE_NODE_ID.values, 
                                                  test_size=0.2, 
                                                  random_state=42, 
                                                  stratify=X_train.BROWSE_NODE_ID.values)
X_train['data_type'] = ['not_set']*X_train.shape[0]

X_train.loc[X_train['COMBINE'].isin(x_train), 'data_type'] = 'train'
X_train.loc[X_train['COMBINE'].isin(x_val), 'data_type'] = 'val'

X_train.groupby(['BROWSE_NODE_ID', 'data_type']).count()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', 
                                          do_lower_case=True)

print('moving next!')

encoded_data_train = tokenizer.batch_encode_plus(
    X_train[X_train.data_type=='train'].COMBINE.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=50, 
    return_tensors='pt'
)

print('moving next!')

encoded_data_val = tokenizer.batch_encode_plus(
    X_train[X_train.data_type=='val'].COMBINE.values, 
    add_special_tokens=True, 
    return_attention_mask=True, 
    pad_to_max_length=True, 
    max_length=50, 
    return_tensors='pt'
)

print('moving next!')

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(X_train[X_train.data_type=='train'].BROWSE_NODE_ID.values)

print('moving next!')

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(X_train[X_train.data_type=='val'].BROWSE_NODE_ID.values)

print('moving next!')

dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)

print('completed!')

label_num = X_train[X_train.data_type=='train'].BROWSE_NODE_ID.max()

label_num

model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=label_num+1,
                                                      output_attentions=False,
                                                      output_hidden_states=False)

batch_size = 32

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

dataloader_validation = DataLoader(dataset_val, 
                                   sampler=SequentialSampler(dataset_val), 
                                   batch_size=batch_size)

optimizer = AdamW(model.parameters(),
                  lr=1e-5, 
                  eps=1e-8)
                  
epochs = 15

scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average='weighted')

def accuracy_per_class(preds, labels):
    label_dict_inverse = {v: k for k, v in label_dict.items()}
    
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

def evaluate(dataloader_val):

    model.eval()
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in dataloader_val:
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals
    
for epoch in tqdm(range(1, epochs+1)):
    
    model.train()
    
    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2]
                 }       

        outputs = model(**inputs)
        
        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
         
        
    torch.save(model.state_dict(), f"drive/MyDrive/data/BERT_epoch_{epoch}_{loss.item()/len(batch)}.pt")
        
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)            
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')













